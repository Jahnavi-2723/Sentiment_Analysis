{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1303c769-9469-40e1-8836-f296b2719008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\guntu\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: keras in c:\\users\\guntu\\anaconda3\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\guntu\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\guntu\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\guntu\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\guntu\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: rich in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from keras) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "#For the model to be trained, we need to install required libraries.\n",
    "!pip install tensorflow keras numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77aebc8a-9324-40ee-ac2a-f33e558a38f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "neutral     835\n",
      "positive    835\n",
      "negative    835\n",
      "Name: count, dtype: int64\n",
      "✅ Balanced dataset saved as 'balanced_reddit_comments.csv'!\n"
     ]
    }
   ],
   "source": [
    "#This snippet of code helps to balance our labelled data \n",
    "\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"labeled_reddit_comments.csv\")\n",
    "\n",
    "# Separate classes\n",
    "df_positive = df[df[\"Sentiment\"] == \"positive\"]\n",
    "df_negative = df[df[\"Sentiment\"] == \"negative\"]\n",
    "df_neutral = df[df[\"Sentiment\"] == \"neutral\"]\n",
    "\n",
    "# Oversample Negative & Neutral to match 835 samples\n",
    "df_negative_oversampled = resample(df_negative, replace=True, n_samples=835, random_state=42)\n",
    "df_neutral_oversampled = resample(df_neutral, replace=True, n_samples=835, random_state=42)\n",
    "\n",
    "# Combine all classes\n",
    "df_balanced = pd.concat([df_positive, df_negative_oversampled, df_neutral_oversampled])\n",
    "\n",
    "# Shuffle data\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Save the balanced dataset\n",
    "df_balanced.to_csv(\"balanced_reddit_comments.csv\", index=False)\n",
    "\n",
    "# Check the new distribution\n",
    "print(df_balanced[\"Sentiment\"].value_counts())\n",
    "print(\"✅ Balanced dataset saved as 'balanced_reddit_comments.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4a6ee092-d13b-4fbf-b760-18418b9ef445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guntu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 437ms/step - accuracy: 0.3991 - loss: 1.0783 - val_accuracy: 0.4990 - val_loss: 0.9882\n",
      "Epoch 2/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 411ms/step - accuracy: 0.4951 - loss: 0.9875 - val_accuracy: 0.5549 - val_loss: 0.9162\n",
      "Epoch 3/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 343ms/step - accuracy: 0.5952 - loss: 0.8209 - val_accuracy: 0.6707 - val_loss: 0.7405\n",
      "Epoch 4/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 426ms/step - accuracy: 0.7938 - loss: 0.5127 - val_accuracy: 0.7465 - val_loss: 0.6225\n",
      "Epoch 5/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 353ms/step - accuracy: 0.8992 - loss: 0.2892 - val_accuracy: 0.7665 - val_loss: 0.6362\n",
      "Epoch 6/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 324ms/step - accuracy: 0.9465 - loss: 0.1515 - val_accuracy: 0.7505 - val_loss: 0.6650\n",
      "Epoch 7/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 384ms/step - accuracy: 0.9769 - loss: 0.0789 - val_accuracy: 0.8084 - val_loss: 0.6623\n",
      "Epoch 8/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 311ms/step - accuracy: 0.9857 - loss: 0.0460 - val_accuracy: 0.7824 - val_loss: 0.7456\n",
      "Epoch 9/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 401ms/step - accuracy: 0.9903 - loss: 0.0471 - val_accuracy: 0.7964 - val_loss: 0.7293\n",
      "Epoch 10/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 355ms/step - accuracy: 0.9971 - loss: 0.0182 - val_accuracy: 0.7784 - val_loss: 0.8611\n",
      "Epoch 11/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 401ms/step - accuracy: 0.9941 - loss: 0.0192 - val_accuracy: 0.8144 - val_loss: 0.8251\n",
      "Epoch 12/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 375ms/step - accuracy: 0.9972 - loss: 0.0130 - val_accuracy: 0.7764 - val_loss: 0.8813\n",
      "Epoch 13/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 327ms/step - accuracy: 0.9977 - loss: 0.0115 - val_accuracy: 0.8024 - val_loss: 0.9313\n",
      "Epoch 14/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 416ms/step - accuracy: 0.9956 - loss: 0.0085 - val_accuracy: 0.8044 - val_loss: 0.9302\n",
      "Epoch 15/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 357ms/step - accuracy: 0.9970 - loss: 0.0106 - val_accuracy: 0.7964 - val_loss: 0.9617\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - accuracy: 0.7991 - loss: 1.0209\n",
      " LSTM Model Accuracy: 0.7964\n"
     ]
    }
   ],
   "source": [
    "#Training the model and finding accuracies with 15 epochs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")  # Use updated dataset\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df[\"Comment\"])\n",
    "X = tokenizer.texts_to_sequences(df[\"Comment\"])\n",
    "X = pad_sequences(X, maxlen=50)  # Pad sequences to same length\n",
    "\n",
    "# Convert Sentiment labels to categorical\n",
    "y = pd.get_dummies(df[\"Sentiment\"]).values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM Model\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 256, input_length=60),\n",
    "    SpatialDropout1D(0.4),  # Increased dropout to prevent overfitting\n",
    "    Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.4)),  \n",
    "    Dense(64, activation=\"relu\"),  \n",
    "    Dropout(0.4),  # Regularization to prevent overfitting\n",
    "    Dense(3, activation=\"softmax\")  \n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate Model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\" LSTM Model Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3c530e1-184a-4052-abe8-cf1ec0f22e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved as 'lstm_sentiment_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Save trained LSTM model\n",
    "model.save(\"lstm_sentiment_model.h5\")\n",
    "print(\" Model saved as 'lstm_sentiment_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b9a14710-a951-4cf1-a995-243c2491c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained LSTM model\n",
    "model = tf.keras.models.load_model(\"lstm_sentiment_model.h5\")  # Ensure the file exists\n",
    "\n",
    "# Load dataset for tokenization (Use the same dataset used during training)\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")  # Use updated dataset\n",
    "  # Load dataset\n",
    "tokenizer = Tokenizer(num_words=5000)  \n",
    "tokenizer.fit_on_texts(df[\"Comment\"])  # Fit tokenizer on training data\n",
    "\n",
    "print(\"Model & Tokenizer Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04ca4daa-51ac-4f32-94ef-07686a5291fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000262B137B4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x00000262B137B4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n",
      "Raw Softmax Probabilities: [[0.31326613 0.02186757 0.66486627]]\n",
      "Predicted Sentiment: Neutral\n",
      "Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "Raw Softmax Probabilities: [[0.05294358 0.4684852  0.4785713 ]]\n",
      "Predicted Sentiment: Neutral\n",
      "Neutral\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_sentiment_model.h5\")\n",
    "\n",
    "# Load tokenizer (use the same dataset for consistency)\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df[\"Comment\"])  # Ensure same tokenization as training\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=60)\n",
    "\n",
    "    probabilities = model.predict(padded_sequence)\n",
    "    sentiment_labels = [\"Negative\", \"Positive\", \"Neutral\"]\n",
    "    \n",
    "    print(\"Raw Softmax Probabilities:\", probabilities)  # Debug softmax outputs\n",
    "    print(\"Predicted Sentiment:\", sentiment_labels[np.argmax(probabilities)])\n",
    "\n",
    "    return sentiment_labels[np.argmax(probabilities)]\n",
    "\n",
    "# Test with clear negative & positive examples\n",
    "print(predict_sentiment(\"This is absolutely terrible. I hate it!\"))  # Should be Negative\n",
    "print(predict_sentiment(\"I am so happy, this is amazing!\"))  # Should be Positive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af109119-fb63-4970-be6c-17800e5173b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "neutral     835\n",
      "positive    835\n",
      "negative    835\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e40f5-211c-4dd1-a4d0-a7479eb0ab1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
