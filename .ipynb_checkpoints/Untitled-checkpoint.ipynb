{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe23405-5be7-4731-9de7-605bef9cef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    words = word_tokenize(text)  # Tokenize words\n",
    "    words = [word for word in words if word.isalnum()]  # Remove punctuation\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization\n",
    "    return \" \".join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524a831-b9ed-4cea-8fab-24f275d1e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model and finding accuracies with 15 epochs\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")  # Use updated dataset\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df[\"Comment\"])\n",
    "X = tokenizer.texts_to_sequences(df[\"Comment\"])\n",
    "X = pad_sequences(X, maxlen=50)  # Pad sequences to same length\n",
    "\n",
    "# Convert Sentiment labels to categorical\n",
    "y = pd.get_dummies(df[\"Sentiment\"]).values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build LSTM Model\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(10000, 256, input_length=60),\n",
    "    SpatialDropout1D(0.4),  # Increased dropout to prevent overfitting\n",
    "    Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.4)),  \n",
    "    Dense(64, activation=\"relu\"),  \n",
    "    Dropout(0.4),  # Regularization to prevent overfitting\n",
    "    Dense(3, activation=\"softmax\")  \n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate Model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\" LSTM Model Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb69375-4ab9-4ed0-b03c-9126d2fa510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"lstm_sentiment_model.h5\")  # Change filename if different\n",
    "\n",
    "# Load tokenizer (Use the same tokenizer from training)\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")  # Use updated dataset\n",
    "  # Load dataset for tokenization\n",
    "tokenizer = Tokenizer(num_words=5000)  \n",
    "tokenizer.fit_on_texts(df[\"Comment\"])  # Fit on training data\n",
    "\n",
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])  # Convert text to sequence\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=50)  # Pad sequence\n",
    "    prediction = model.predict(padded_sequence)  # Predict sentiment\n",
    "\n",
    "    sentiment_labels = [\"Negative\", \"Positive\", \"Neutral\"]\n",
    "    predicted_label = sentiment_labels[np.argmax(prediction)]  # Get highest probability label\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Test with user input\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter a sentence to analyze sentiment (or type 'exit' to stop): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    sentiment = predict_sentiment(user_input)\n",
    "    print(f\"ðŸ”¹ Predicted Sentiment: {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
