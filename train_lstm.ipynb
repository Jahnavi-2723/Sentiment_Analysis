{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1303c769-9469-40e1-8836-f296b2719008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\guntu\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: keras in c:\\users\\guntu\\anaconda3\\lib\\site-packages (3.9.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\guntu\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\guntu\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\guntu\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\guntu\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: rich in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from keras) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\guntu\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "#For the model to be trained, we need to install required libraries.\n",
    "!pip install tensorflow keras numpy pandas scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77aebc8a-9324-40ee-ac2a-f33e558a38f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guntu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guntu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guntu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment\n",
      "neutral     835\n",
      "negative    835\n",
      "positive    551\n",
      "Name: count, dtype: int64\n",
      " Balanced & Preprocessed dataset saved as 'balanced_reddit_comments_updated.csv'!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"labeled_reddit_comments.csv\")  # Original labeled dataset\n",
    "\n",
    "# Separate classes\n",
    "df_positive = df[df[\"Sentiment\"] == \"positive\"]\n",
    "df_negative = df[df[\"Sentiment\"] == \"negative\"]\n",
    "df_neutral = df[df[\"Sentiment\"] == \"neutral\"]\n",
    "\n",
    "# Oversample Negative & Neutral to match 835 samples\n",
    "df_negative_oversampled = resample(df_negative, replace=True, n_samples=835, random_state=42)\n",
    "df_neutral_oversampled = resample(df_neutral, replace=True, n_samples=835, random_state=42)\n",
    "\n",
    "# Combine all classes\n",
    "df_balanced = pd.concat([df_positive, df_negative_oversampled, df_neutral_oversampled])\n",
    "\n",
    "# Shuffle data\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# **Apply preprocessing to text before saving**\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):  \n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Handle negations like \"not good\" → \"not_good\"\n",
    "    negation_words = {\"not\", \"no\", \"never\", \"n't\"}\n",
    "    processed_words = []\n",
    "    i = 0\n",
    "    while i < len(words):\n",
    "        if words[i] in negation_words and i + 1 < len(words):\n",
    "            processed_words.append(words[i] + \"_\" + words[i + 1])\n",
    "            i += 1  \n",
    "        else:\n",
    "            processed_words.append(words[i])\n",
    "        i += 1\n",
    "\n",
    "    words = [lemmatizer.lemmatize(word) for word in processed_words if word.isalnum() and word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "# **Apply preprocessing**\n",
    "df_balanced[\"Processed_Comment\"] = df_balanced[\"Comment\"].apply(preprocess_text)\n",
    "\n",
    "# Save the **fully processed & balanced dataset**\n",
    "df_balanced.to_csv(\"balanced_reddit_comments_updated.csv\", index=False)\n",
    "\n",
    "# Check the new distribution\n",
    "print(df_balanced[\"Sentiment\"].value_counts())\n",
    "print(\" Balanced & Preprocessed dataset saved as 'balanced_reddit_comments_updated.csv'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a6ee092-d13b-4fbf-b760-18418b9ef445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guntu\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 188ms/step - accuracy: 0.3723 - loss: 1.0839 - val_accuracy: 0.4571 - val_loss: 1.0243\n",
      "Epoch 2/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 143ms/step - accuracy: 0.5293 - loss: 0.9644 - val_accuracy: 0.5609 - val_loss: 0.8736\n",
      "Epoch 3/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 144ms/step - accuracy: 0.6346 - loss: 0.7631 - val_accuracy: 0.7585 - val_loss: 0.6383\n",
      "Epoch 4/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 150ms/step - accuracy: 0.8591 - loss: 0.3843 - val_accuracy: 0.7864 - val_loss: 0.5530\n",
      "Epoch 5/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 180ms/step - accuracy: 0.9426 - loss: 0.1846 - val_accuracy: 0.8084 - val_loss: 0.6007\n",
      "Epoch 6/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 172ms/step - accuracy: 0.9715 - loss: 0.1014 - val_accuracy: 0.8343 - val_loss: 0.6131\n",
      "Epoch 7/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 168ms/step - accuracy: 0.9818 - loss: 0.0623 - val_accuracy: 0.8184 - val_loss: 0.6558\n",
      "Epoch 8/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 162ms/step - accuracy: 0.9889 - loss: 0.0402 - val_accuracy: 0.8303 - val_loss: 0.7238\n",
      "Epoch 9/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 166ms/step - accuracy: 0.9930 - loss: 0.0274 - val_accuracy: 0.8224 - val_loss: 0.7307\n",
      "Epoch 10/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 158ms/step - accuracy: 0.9949 - loss: 0.0217 - val_accuracy: 0.8323 - val_loss: 0.7993\n",
      "Epoch 11/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9932 - loss: 0.0202 - val_accuracy: 0.8283 - val_loss: 0.8683\n",
      "Epoch 12/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 160ms/step - accuracy: 0.9921 - loss: 0.0207 - val_accuracy: 0.8204 - val_loss: 0.8469\n",
      "Epoch 13/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9995 - loss: 0.0095 - val_accuracy: 0.8263 - val_loss: 0.9204\n",
      "Epoch 14/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 159ms/step - accuracy: 0.9980 - loss: 0.0094 - val_accuracy: 0.8263 - val_loss: 1.0171\n",
      "Epoch 15/15\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 160ms/step - accuracy: 0.9977 - loss: 0.0076 - val_accuracy: 0.8224 - val_loss: 0.9647\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.8272 - loss: 0.9110\n",
      " Fine-Tuned LSTM Model Accuracy: 0.8224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, Bidirectional, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")  #  Use updated dataset\n",
    "\n",
    "# Convert to string and handle NaN values\n",
    "df[\"Processed_Comment\"] = df[\"Processed_Comment\"].astype(str).fillna(\"\")\n",
    "\n",
    "# **Use Processed_Comment**\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(df[\"Processed_Comment\"])  #  Use preprocessed text\n",
    "X = tokenizer.texts_to_sequences(df[\"Processed_Comment\"])  #  Use preprocessed text\n",
    "X = pad_sequences(X, maxlen=50)  # Pad sequences to same length\n",
    "\n",
    "# Convert Sentiment labels to categorical\n",
    "y = pd.get_dummies(df[\"Sentiment\"]).values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# **Fine-Tuned LSTM Model**\n",
    "model = Sequential([\n",
    "    Embedding(10000, 256, input_length=60),\n",
    "    SpatialDropout1D(0.4),  \n",
    "    Bidirectional(LSTM(128, dropout=0.4, recurrent_dropout=0.4)),  \n",
    "    Dense(64, activation=\"relu\"),  \n",
    "    Dropout(0.4),  \n",
    "    Dense(3, activation=\"softmax\")  \n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluate Model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\" Fine-Tuned LSTM Model Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a3c530e1-184a-4052-abe8-cf1ec0f22e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model saved as 'lstm_sentiment_model.h5'\n"
     ]
    }
   ],
   "source": [
    "# Save trained LSTM model\n",
    "model.save(\"lstm_sentiment_model.h5\")\n",
    "print(\" Model saved as 'lstm_sentiment_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b9a14710-a951-4cf1-a995-243c2491c62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model & Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained LSTM model\n",
    "model = tf.keras.models.load_model(\"lstm_sentiment_model.h5\")  # Ensure the file exists\n",
    "\n",
    "# Load dataset for tokenization (Use the same dataset used during training)\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")  # Use updated dataset\n",
    "  # Load dataset\n",
    "tokenizer = Tokenizer(num_words=5000)  \n",
    "df[\"Processed_Comment\"] = df[\"Processed_Comment\"].astype(str).fillna(\"\")\n",
    "tokenizer.fit_on_texts(df[\"Processed_Comment\"])  # Fit tokenizer on training data\n",
    "\n",
    "print(\"Model & Tokenizer Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04ca4daa-51ac-4f32-94ef-07686a5291fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 784ms/step\n",
      "Raw Softmax Probabilities: [[1.8276625e-06 4.9813181e-10 9.9999821e-01]]\n",
      "Predicted Sentiment: Neutral\n",
      "Neutral\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Raw Softmax Probabilities: [[9.9858618e-01 2.8892771e-05 1.3848791e-03]]\n",
      "Predicted Sentiment: Negative\n",
      "Negative\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load trained model\n",
    "model = tf.keras.models.load_model(\"lstm_sentiment_model.h5\")\n",
    "\n",
    "# Load tokenizer (use the same dataset for consistency)\n",
    "df = pd.read_csv(\"balanced_reddit_comments_updated.csv\")\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df[\"Comment\"])  # Ensure same tokenization as training\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=60)\n",
    "\n",
    "    probabilities = model.predict(padded_sequence)\n",
    "    sentiment_labels = [\"Negative\", \"Positive\", \"Neutral\"]\n",
    "    \n",
    "    print(\"Raw Softmax Probabilities:\", probabilities)  # Debug softmax outputs\n",
    "    print(\"Predicted Sentiment:\", sentiment_labels[np.argmax(probabilities)])\n",
    "\n",
    "    return sentiment_labels[np.argmax(probabilities)]\n",
    "\n",
    "# Test with clear negative & positive examples\n",
    "print(predict_sentiment(\"It's okay, nothing too great or too bad.\"))  # Should be Neutral\n",
    "print(predict_sentiment(\"This is the worst experience ever. I regret buying this.\"))  # Should be negative\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e40f5-211c-4dd1-a4d0-a7479eb0ab1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
